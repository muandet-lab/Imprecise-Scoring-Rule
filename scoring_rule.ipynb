{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff292ec1-d778-4a2c-a505-4e5a66797bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "#%matplotlib notebook\n",
    "torch.manual_seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "path = os.path.join(os.getcwd(), \"plot/\")\n",
    "today=datetime.today().strftime('%Y-%m-%d')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d01769-7081-40f3-9c14-4b17817e518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized q*: 0.7000 (True p: 0.7)\n",
      "Elicitor sampled y = 0.0, Feedback = 0.4900\n"
     ]
    }
   ],
   "source": [
    "class OptimizerAgent:\n",
    "    def __init__(self, true_p, lr=0.1, q_init=0.5):\n",
    "        \"\"\"\n",
    "        The optimizer knows the true p and chooses q* to maximize expected reward.\n",
    "        :param true_p: The true probability of success.\n",
    "        :param lr: Learning rate for optimizing q.\n",
    "        :param q_init: Initial guess for the probability to report.\n",
    "        \"\"\"\n",
    "        self.true_p = true_p\n",
    "        self.q = torch.tensor([q_init], requires_grad=True)  # Trainable q\n",
    "        self.optimizer = torch.optim.SGD([self.q], lr=lr)\n",
    "\n",
    "    def quadratic_loss(self, q, y):\n",
    "        \"\"\"Quadratic scoring rule loss.\"\"\"\n",
    "        return (y - q) ** 2\n",
    "\n",
    "    def expected_loss(self, q):\n",
    "        \"\"\"\n",
    "        Compute the expected loss given the true p.\n",
    "        E[loss] = p * loss(q, 0) + (1 - p) * loss(q, 1)\n",
    "        \"\"\"\n",
    "        loss_0 = self.quadratic_loss(q, torch.tensor(0.0))  # Loss if outcome is 0\n",
    "        loss_1 = self.quadratic_loss(q, torch.tensor(1.0))  # Loss if outcome is 1\n",
    "        return (1-self.true_p) * loss_0 + self.true_p * loss_1\n",
    "\n",
    "    def optimize_q(self, steps=100):\n",
    "        \"\"\"Optimize q* to minimize expected loss.\"\"\"\n",
    "        for _ in range(steps):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.expected_loss(self.q)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.q.data.clamp_(0, 1)  # Keep q in [0,1]\n",
    "        return self.q.item()\n",
    "\n",
    "class No_Decision_Agent:\n",
    "    def __init__(self, q):\n",
    "        \"\"\"\n",
    "        The elicitor receives q* and samples an outcome.\n",
    "        :param q: The optimized probability from the optimizer agent.\n",
    "        \"\"\"\n",
    "        self.q = q\n",
    "\n",
    "    def sample_outcome(self):\n",
    "        \"\"\"Sample y ~ Bernoulli(q).\"\"\"\n",
    "        return torch.bernoulli(torch.tensor(self.q)).item()\n",
    "\n",
    "    def provide_feedback(self, y):\n",
    "        \"\"\"Provide feedback using the negative quadratic loss.\"\"\"\n",
    "        return ((y - self.q) ** 2)\n",
    "\n",
    "# Define true p\n",
    "true_p = 0.7\n",
    "\n",
    "# Step 1: Optimizer chooses q* to maximize expected reward\n",
    "optimizer = OptimizerAgent(true_p, lr=0.1)\n",
    "q_star = optimizer.optimize_q()\n",
    "print(f\"Optimized q*: {q_star:.4f} (True p: {true_p})\")\n",
    "\n",
    "# Step 2: Elicitor receives q*, samples y, and provides feedback\n",
    "elicitor = No_Decision_Agent(q_star)\n",
    "y_sampled = elicitor.sample_outcome()\n",
    "feedback = elicitor.provide_feedback(y_sampled)\n",
    "\n",
    "print(f\"Elicitor sampled y = {y_sampled}, Feedback = {feedback:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2a94a5-5a80-4427-ad5b-90082b1302bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized q*: 0.0419 0.1801 (True p: 0.05 0.15000000000000002)\n",
      "Elicitor sampled y = 0.0, Feedback = 0.0123\n",
      "Elicitor sampled y = 0.0, Feedback = 0.0100\n",
      "Elicitor sampled y = 0.0, Feedback = 0.0100\n",
      "Elicitor sampled y = 0.0, Feedback = 0.2500\n"
     ]
    }
   ],
   "source": [
    "## Randomized Aggregation setup\n",
    "class ImpreciseOptimizerAgent:\n",
    "    def __init__(self, mu, sigma, aggregation, lr=0.1, report_params_init=[0.5,0.2]):\n",
    "        \"\"\"\n",
    "        The optimizer knows the true p and chooses q* to maximize expected reward.\n",
    "        :param true_p: The true probability of success.\n",
    "        :param lr: Learning rate for optimizing q.\n",
    "        :param q_init: Initial guess for the probability to report.\n",
    "        \"\"\"\n",
    "        self.true_mu, self.true_sigma = mu, sigma\n",
    "        self.aggregation = aggregation\n",
    "        self.report_raw_mu = torch.tensor([report_params_init[0]], requires_grad=True)\n",
    "        self.report_raw_sigma = torch.tensor([report_params_init[0]], requires_grad=True)\n",
    "        self.optimizer = torch.optim.SGD([self.report_raw_mu, self.report_raw_sigma], lr=lr)\n",
    "\n",
    "    def quadratic_loss(self, q, y):\n",
    "        \"\"\"Quadratic scoring rule loss.\"\"\"\n",
    "        return (y - q) ** 2\n",
    "\n",
    "    def expected_loss(self, report_mu, report_sigma):\n",
    "        \"\"\"\n",
    "        Compute the expected loss given the true p.\n",
    "        E[loss] = p * loss(q, 0) + (1 - p) * loss(q, 1)\n",
    "        \"\"\"\n",
    "        true_p = self.aggregation*(self.true_mu-self.true_sigma)+(1-self.aggregation)*(self.true_mu+self.true_sigma)\n",
    "        # closed form solution to a\\in [0,1] and u:A X Y -> R being a strictly convex function like (y-a)**2 \n",
    "        a_star_q = self.aggregation*(report_mu-report_sigma)+(1-self.aggregation)*(report_mu+report_sigma) \n",
    "        loss_0 = self.quadratic_loss(a_star_q, torch.tensor(0.0))  # Loss if outcome is 0\n",
    "        loss_1 = self.quadratic_loss(a_star_q, torch.tensor(1.0))  # Loss if outcome is 1\n",
    "        return (1-true_p) * loss_0 + true_p * loss_1\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Reparameterize raw_mu and raw_sigma to enforce:\n",
    "          - μ ∈ (0,1) via a sigmoid,\n",
    "          - σ ∈ [0, min(μ, 1-μ)] by scaling another sigmoid.\n",
    "        \"\"\"\n",
    "        # Force μ into (0,1)\n",
    "        mu = torch.sigmoid(self.report_raw_mu)\n",
    "        # Determine maximum σ allowed so that μ-σ >= 0 and μ+σ <= 1\n",
    "        sigma_max = torch.min(mu, 1 - mu)\n",
    "        # Force σ into [0, sigma_max]\n",
    "        sigma = torch.sigmoid(self.report_raw_sigma) * sigma_max\n",
    "        return mu, sigma\n",
    "    \n",
    "    def optimize_IP(self, steps=1000):\n",
    "        \"\"\"Optimize q* to minimize expected loss.\"\"\"\n",
    "        for _ in range(steps):\n",
    "            self.optimizer.zero_grad()\n",
    "            report_mu, report_sigma = self.get_params()\n",
    "            loss = self.expected_loss(report_mu, report_sigma)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return [item.item() for item in self.get_params()]\n",
    "\n",
    "class No_Decision_Agent:\n",
    "    def __init__(self, mu, sigma, aggregation=0.5):\n",
    "        \"\"\"\n",
    "        The elicitor receives q* and samples an outcome.\n",
    "        :param q: The optimized probability from the optimizer agent.\n",
    "        \"\"\"\n",
    "        self.nature = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1) # random outcome from nature \n",
    "        self.aggregation = aggregation\n",
    "        self.mu, self.sigma = mu, sigma\n",
    "        self.a = self.aggregation*(self.mu-self.sigma)+(1-self.aggregation)*(self.mu+self.sigma) \n",
    "        self.outcome = self.sample_outcome()\n",
    "        \n",
    "    def sample_outcome(self):\n",
    "        \"\"\"Sample y ~ Bernoulli(q).\"\"\"\n",
    "        return torch.bernoulli(torch.tensor(self.nature)).item()\n",
    "    \n",
    "    def update_the_reports(self, mu, sigma):\n",
    "        self.mu, self.sigma = mu, sigma\n",
    "        self.a = self.aggregation*(self.mu-self.sigma)+(1-self.aggregation)*(self.mu+self.sigma)\n",
    "        return \n",
    "    \n",
    "    def provide_feedback(self):\n",
    "        \"\"\"Provide feedback using the negative quadratic loss.\"\"\"\n",
    "        return ((self.outcome - self.a) ** 2)\n",
    "\n",
    "# Define true p\n",
    "true_mu, true_sigma = 0.1, 0.05\n",
    "aggregation = 0.5\n",
    "# Step 1: Optimizer chooses q* to maximize expected reward\n",
    "optimizer = ImpreciseOptimizerAgent(mu=true_mu,sigma=true_sigma, aggregation=aggregation, lr=0.1)\n",
    "q_star = optimizer.optimize_IP()\n",
    "print(f\"Optimized q*: {q_star[0]-q_star[1]:.4f} {q_star[0]+q_star[1]:.4f} (True p: {true_mu-true_sigma} {true_mu+true_sigma})\")\n",
    "\n",
    "# Step 2: Elicitor receives q*, samples y, and provides feedback\n",
    "elicitor = No_Decision_Agent(q_star[0],q_star[1], aggregation)\n",
    "feedback = elicitor.provide_feedback()\n",
    "\n",
    "print(f\"Elicitor sampled y = {elicitor.outcome}, Feedback = {feedback:.4f}\")\n",
    "\n",
    "elicitor.update_the_reports(0.1, 0.0)\n",
    "feedback = elicitor.provide_feedback()\n",
    "print(f\"Elicitor sampled y = {elicitor.outcome}, Feedback = {feedback:.4f}\")\n",
    "\n",
    "elicitor.update_the_reports(0.1, 0.05)\n",
    "feedback = elicitor.provide_feedback()\n",
    "print(f\"Elicitor sampled y = {elicitor.outcome}, Feedback = {feedback:.4f}\")\n",
    "\n",
    "elicitor.update_the_reports(0.5, 0.5)\n",
    "feedback = elicitor.provide_feedback()\n",
    "print(f\"Elicitor sampled y = {elicitor.outcome}, Feedback = {feedback:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b78f68-9401-4a59-a849-b608b475e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized q*: 0.0430 0.1547 (True p: 0.05 0.15000000000000002)\n",
      "Elicitor sampled y = 0.0, Feedback = 0.0098\n"
     ]
    }
   ],
   "source": [
    "## Randomized Aggregation setup with [μ-σ, μ+σ]\n",
    "class ImpreciseOptimizerAgent:\n",
    "    def __init__(self, mu, sigma, lr=0.1, report_params_init=[0.5,0.2]):\n",
    "        \"\"\"\n",
    "        The optimizer knows the true p and chooses q* to maximize expected reward.\n",
    "        :param true_p: The true probability of success.\n",
    "        :param lr: Learning rate for optimizing q.\n",
    "        :param q_init: Initial guess for the probability to report.\n",
    "        \"\"\"\n",
    "        self.true_mu, self.true_sigma = mu, sigma\n",
    "        self.report_raw_mu = torch.tensor([report_params_init[0]], requires_grad=True)\n",
    "        self.report_raw_sigma = torch.tensor([report_params_init[0]], requires_grad=True)\n",
    "        self.optimizer = torch.optim.SGD([self.report_raw_mu, self.report_raw_sigma], lr=lr)\n",
    "\n",
    "    def quadratic_loss(self, q, y):\n",
    "        \"\"\"Quadratic scoring rule loss.\"\"\"\n",
    "        return (y - q) ** 2\n",
    "\n",
    "    def expected_loss(self, report_mu, report_sigma, aggregation):\n",
    "        \"\"\"\n",
    "        Compute the expected loss given the true p.\n",
    "        E[loss] = p * loss(q, 0) + (1 - p) * loss(q, 1)\n",
    "        \"\"\"\n",
    "        true_p = aggregation*(self.true_mu-self.true_sigma)+(1-aggregation)*(self.true_mu+self.true_sigma)\n",
    "        # closed form solution to a\\in [0,1] and u:A X Y -> R being a strictly convex function like (y-a)**2 \n",
    "        a_star_q = aggregation*(report_mu-report_sigma)+(1-aggregation)*(report_mu+report_sigma) \n",
    "        loss_0 = self.quadratic_loss(a_star_q, torch.tensor(0.0))  # Loss if outcome is 0\n",
    "        loss_1 = self.quadratic_loss(a_star_q, torch.tensor(1.0))  # Loss if outcome is 1\n",
    "        return (1-true_p) * loss_0 + true_p * loss_1\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Reparameterize raw_mu and raw_sigma to enforce:\n",
    "          - μ ∈ (0,1) via a sigmoid,\n",
    "          - σ ∈ [0, min(μ, 1-μ)] by scaling another sigmoid.\n",
    "        \"\"\"\n",
    "        # Force μ into (0,1)\n",
    "        mu = torch.sigmoid(self.report_raw_mu)\n",
    "        # Determine maximum σ allowed so that μ-σ >= 0 and μ+σ <= 1\n",
    "        sigma_max = torch.min(mu, 1 - mu)\n",
    "        # Force σ into [0, sigma_max]\n",
    "        sigma = torch.sigmoid(self.report_raw_sigma) * sigma_max\n",
    "        return mu, sigma\n",
    "    \n",
    "    def optimize_IP(self, steps=100):\n",
    "        \"\"\"Optimize q* to minimize expected loss.\"\"\"\n",
    "        for _ in range(steps):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for aggregation in torch.rand(100):\n",
    "                report_mu, report_sigma = self.get_params()\n",
    "                loss += self.expected_loss(report_mu, report_sigma, aggregation)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return [item.item() for item in self.get_params()]\n",
    "\n",
    "class No_Decision_Agent:\n",
    "    def __init__(self, mu, sigma, aggregation=0.5):\n",
    "        \"\"\"\n",
    "        The elicitor receives q* and samples an outcome.\n",
    "        :param q: The optimized probability from the optimizer agent.\n",
    "        \"\"\"\n",
    "        self.nature = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1) # random outcome from nature \n",
    "        self.aggregation = aggregation\n",
    "        self.mu, self.sigma = mu, sigma\n",
    "        self.a = self.aggregation*(self.mu-self.sigma)+(1-self.aggregation)*(self.mu+self.sigma) \n",
    "    def sample_outcome(self):\n",
    "        \"\"\"Sample y ~ Bernoulli(q).\"\"\"\n",
    "        return torch.bernoulli(torch.tensor(self.nature)).item()\n",
    "\n",
    "    def provide_feedback(self, y):\n",
    "        \"\"\"Provide feedback using the negative quadratic loss.\"\"\"\n",
    "        return ((y - self.a) ** 2)\n",
    "\n",
    "# Define true p\n",
    "true_mu, true_sigma = 0.1, 0.05\n",
    "aggregation = 0.5\n",
    "# Step 1: Optimizer chooses q* to maximize expected reward\n",
    "optimizer = ImpreciseOptimizerAgent(mu=true_mu,sigma=true_sigma, lr=0.1)\n",
    "q_star = optimizer.optimize_IP()\n",
    "print(f\"Optimized q*: {q_star[0]-q_star[1]:.4f} {q_star[0]+q_star[1]:.4f} (True p: {true_mu-true_sigma} {true_mu+true_sigma})\")\n",
    "\n",
    "# Step 2: Elicitor receives q*, samples y, and provides feedback\n",
    "elicitor = No_Decision_Agent(q_star[0],q_star[1], aggregation)\n",
    "y_sampled = elicitor.sample_outcome()\n",
    "feedback = elicitor.provide_feedback(y_sampled)\n",
    "\n",
    "print(f\"Elicitor sampled y = {y_sampled}, Feedback = {feedback:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1dee12b-8101-4585-838c-b9ff252157bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized q*: 0.0500 0.1500 (True p: 0.05 0.15)\n",
      "Elicitor sampled y = 0.0, Feedback = 0.0100\n"
     ]
    }
   ],
   "source": [
    "## Randomized Aggregation setup with p_low and p_up\n",
    "class ImpreciseOptimizerAgent:\n",
    "    def __init__(self, p_low, p_up, lr=0.1, report_params_init=[0.2,0.5]):\n",
    "        \"\"\"\n",
    "        The optimizer knows the true p and chooses q* to maximize expected reward.\n",
    "        :param true_p: The true probability of success.\n",
    "        :param lr: Learning rate for optimizing q.\n",
    "        :param q_init: Initial guess for the probability to report.\n",
    "        \"\"\"\n",
    "        self.p_low, self.p_up = p_low, p_up\n",
    "        self.report_p_low = torch.tensor([report_params_init[0]], requires_grad=True)\n",
    "        self.report_p_up = torch.tensor([report_params_init[0]], requires_grad=True)\n",
    "        self.optimizer = torch.optim.SGD([self.report_p_low, self.report_p_up], lr=lr)\n",
    "\n",
    "    def quadratic_loss(self, q, y):\n",
    "        \"\"\"Quadratic scoring rule loss.\"\"\"\n",
    "        return (y - q) ** 2\n",
    "\n",
    "    def expected_loss(self, report_low, report_up, aggregation):\n",
    "        \"\"\"\n",
    "        Compute the expected loss given the true p.\n",
    "        E[loss] = p * loss(q, 0) + (1 - p) * loss(q, 1)\n",
    "        \"\"\"\n",
    "        true_p = aggregation*(self.p_low)+(1-aggregation)*(self.p_up)\n",
    "        # closed form solution to a\\in [0,1] and u:A X Y -> R being a strictly convex function like (y-a)**2 \n",
    "        a_star_q = aggregation*(report_low)+(1-aggregation)*(report_up) \n",
    "        loss_0 = self.quadratic_loss(a_star_q, torch.tensor(0.0))  # Loss if outcome is 0\n",
    "        loss_1 = self.quadratic_loss(a_star_q, torch.tensor(1.0))  # Loss if outcome is 1\n",
    "        return (1-true_p) * loss_0 + true_p * loss_1\n",
    "    \n",
    "    def get_params(self):\n",
    "        # Force μ into (0,1)\n",
    "        p_low = torch.sigmoid(self.report_p_low)\n",
    "        p_up = torch.sigmoid(self.report_p_up)\n",
    "        return p_low, p_up\n",
    "    \n",
    "    def optimize_IP(self, steps=1000):\n",
    "        \"\"\"Optimize q* to minimize expected loss.\"\"\"\n",
    "        for _ in range(steps):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for aggregation in torch.rand(100):\n",
    "                report_mu, report_sigma = self.get_params()\n",
    "                loss += self.expected_loss(report_mu, report_sigma, aggregation)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return [item.item() for item in self.get_params()]\n",
    "\n",
    "class No_Decision_Agent:\n",
    "    def __init__(self, q_low, q_up, aggregation=0.5):\n",
    "        \"\"\"\n",
    "        The elicitor receives q* and samples an outcome.\n",
    "        :param q: The optimized probability from the optimizer agent.\n",
    "        \"\"\"\n",
    "        self.nature = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1) # random outcome from nature \n",
    "        self.aggregation = aggregation\n",
    "        self.q_low, self.q_up = q_low, q_up\n",
    "        self.a = self.aggregation*(q_low)+(1-self.aggregation)*(q_up) \n",
    "    def sample_outcome(self):\n",
    "        \"\"\"Sample y ~ Bernoulli(q).\"\"\"\n",
    "        return torch.bernoulli(torch.tensor(self.nature)).item()\n",
    "\n",
    "    def provide_feedback(self, y):\n",
    "        \"\"\"Provide feedback using the negative quadratic loss.\"\"\"\n",
    "        return ((y - self.a) ** 2)\n",
    "\n",
    "# Define true p\n",
    "p_low, p_up = 0.05, 0.15\n",
    "# Step 1: Optimizer chooses q* to maximize expected reward\n",
    "optimizer = ImpreciseOptimizerAgent(p_low,p_up, lr=0.1)\n",
    "q_star = optimizer.optimize_IP()\n",
    "print(f\"Optimized q*: {q_star[0]:.4f} {q_star[1]:.4f} (True p: {p_low} {p_up})\")\n",
    "\n",
    "# Step 2: Elicitor receives q*, samples y, and provides feedback\n",
    "elicitor = No_Decision_Agent(q_star[0],q_star[1])\n",
    "y_sampled = elicitor.sample_outcome()\n",
    "feedback = elicitor.provide_feedback(y_sampled)\n",
    "\n",
    "print(f\"Elicitor sampled y = {y_sampled}, Feedback = {feedback:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5143e7-692e-4d0c-9b5a-35d65334e666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
